'use client';

import { useState, useRef, useEffect } from 'react';

export default function Home() {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [aiResponse, setAiResponse] = useState('');
  const [error, setError] = useState('');
  const [volumeLevel, setVolumeLevel] = useState(0);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recordingStartTimeRef = useRef<number>(0);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const volumeSamplesRef = useRef<number[]>([]);

  const startRecording = async () => {
    try {
      setError('');
      setVolumeLevel(0);
      volumeSamplesRef.current = [];

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // Setup Audio Context for volume analysis
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();

      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.8;
      source.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      // Start volume monitoring
      monitorVolume();

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm',
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        // Stop volume monitoring
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current);
          animationFrameRef.current = null;
        }

        // Close audio context
        if (audioContextRef.current) {
          audioContextRef.current.close();
          audioContextRef.current = null;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });

        // Calculate recording duration
        const recordingDuration = Date.now() - recordingStartTimeRef.current;

        // Validate recording (minimum 0.5 seconds)
        if (recordingDuration < 500) {
          setError('éŒ„éŸ³æ™‚é–“å¤ªçŸ­ï¼Œè«‹æŒ‰ä½è‡³å°‘ 0.5 ç§’å¾Œå†èªªè©±');
          stream.getTracks().forEach(track => track.stop());
          setVolumeLevel(0);
          return;
        }

        // Check average volume (threshold: 0.01)
        const avgVolume = volumeSamplesRef.current.reduce((a, b) => a + b, 0) / volumeSamplesRef.current.length;
        if (avgVolume < 0.01) {
          setError('æœªåµæ¸¬åˆ°èªéŸ³ï¼Œè«‹æé«˜éŸ³é‡æˆ–é è¿‘éº¥å…‹é¢¨èªªè©±');
          stream.getTracks().forEach(track => track.stop());
          setVolumeLevel(0);
          return;
        }

        if (audioBlob.size < 5000) {
          setError('éŸ³è¨Šè³‡æ–™ä¸è¶³ï¼Œè«‹ç¢ºèªéº¥å…‹é¢¨æ­£å¸¸é‹ä½œ');
          stream.getTracks().forEach(track => track.stop());
          setVolumeLevel(0);
          return;
        }

        await processAudio(audioBlob);

        // Stop all tracks
        stream.getTracks().forEach(track => track.stop());
        setVolumeLevel(0);
      };

      mediaRecorder.start();
      recordingStartTimeRef.current = Date.now();
      setIsRecording(true);
    } catch (err) {
      console.error('Failed to start recording:', err);
      setError('ç„¡æ³•å•Ÿå‹•éº¥å…‹é¢¨ï¼Œè«‹ç¢ºèªæ¬Šé™è¨­å®š');
    }
  };

  const monitorVolume = () => {
    if (!analyserRef.current) return;

    const analyser = analyserRef.current;
    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const checkVolume = () => {
      analyser.getByteFrequencyData(dataArray);

      // Calculate RMS (Root Mean Square) volume
      let sum = 0;
      for (let i = 0; i < dataArray.length; i++) {
        sum += dataArray[i] * dataArray[i];
      }
      const rms = Math.sqrt(sum / dataArray.length) / 255;

      setVolumeLevel(rms);
      volumeSamplesRef.current.push(rms);

      animationFrameRef.current = requestAnimationFrame(checkVolume);
    };

    checkVolume();
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  // Keyboard shortcut for recording (Space key)
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      // Prevent recording if already processing or recording
      if (isProcessing || isRecording) return;

      // Use Space key to start recording
      if (e.code === 'Space' && !e.repeat) {
        e.preventDefault();
        startRecording();
      }
    };

    const handleKeyUp = (e: KeyboardEvent) => {
      if (e.code === 'Space') {
        e.preventDefault();
        stopRecording();
      }
    };

    window.addEventListener('keydown', handleKeyDown);
    window.addEventListener('keyup', handleKeyUp);

    return () => {
      window.removeEventListener('keydown', handleKeyDown);
      window.removeEventListener('keyup', handleKeyUp);
    };
  }, [isRecording, isProcessing]);

  const processAudio = async (audioBlob: Blob) => {
    setIsProcessing(true);
    setTranscript('');
    setAiResponse('');

    try {
      // Step 1: Speech to Text
      const formData = new FormData();
      formData.append('audio', audioBlob);

      const sttResponse = await fetch('/api/stt', {
        method: 'POST',
        body: formData,
      });

      if (!sttResponse.ok) {
        throw new Error('STT failed');
      }

      const { transcription } = await sttResponse.json();
      setTranscript(transcription);

      // Step 2: Chat Completion
      const chatResponse = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: transcription }),
      });

      if (!chatResponse.ok) {
        throw new Error('Chat failed');
      }

      const { response } = await chatResponse.json();
      setAiResponse(response);

      // Step 3: Text to Speech
      const ttsResponse = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: response }),
      });

      if (!ttsResponse.ok) {
        throw new Error('TTS failed');
      }

      const audioBuffer = await ttsResponse.arrayBuffer();
      const audio = new Audio(URL.createObjectURL(new Blob([audioBuffer], { type: 'audio/mpeg' })));
      await audio.play();

    } catch (err) {
      console.error('Processing error:', err);
      setError('è™•ç†å¤±æ•—ï¼Œè«‹é‡è©¦');
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <main className="flex min-h-screen flex-col items-center justify-center p-8">
      <div className="max-w-2xl w-full space-y-8">
        {/* Header */}
        <div className="text-center">
          <h1 className="text-4xl font-bold mb-2">
            AI Audio Chat Visualizer
          </h1>
          <p className="text-gray-600">
            MVP v1.0 - èªéŸ³å°è©±åŸå‹ï¼ˆå‚³çµ±æ¨¡å¼ï¼‰
          </p>
          <div className="mt-4 p-3 bg-blue-50 border border-blue-200 rounded-lg">
            <p className="text-sm text-blue-700 mb-2">
              âš¡ æ–°åŠŸèƒ½ï¼šæ¥µé€ŸèªéŸ³å°è©±æ¨¡å¼å·²æ¨å‡ºï¼
            </p>
            <a
              href="/realtime"
              className="inline-block px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition-colors"
            >
              ğŸš€ åˆ‡æ›åˆ° Realtime æ¨¡å¼ (å»¶é² &lt; 1 ç§’)
            </a>
          </div>
        </div>

        {/* Microphone Button */}
        <div className="flex flex-col items-center gap-4">
          <button
            onMouseDown={startRecording}
            onMouseUp={stopRecording}
            onTouchStart={startRecording}
            onTouchEnd={stopRecording}
            disabled={isProcessing}
            className={`
              w-32 h-32 rounded-full text-white font-bold text-lg
              transition-all duration-200 transform
              ${isRecording
                ? 'bg-red-500 scale-110 animate-pulse'
                : 'bg-blue-500 hover:scale-105'
              }
              ${isProcessing ? 'opacity-50 cursor-not-allowed' : 'hover:shadow-lg active:scale-95'}
            `}
          >
            {isRecording ? 'ğŸ¤ éŒ„éŸ³ä¸­' : isProcessing ? 'â³ è™•ç†ä¸­' : 'ğŸ¤ æŒ‰ä½èªªè©±'}
          </button>

          {/* Volume Indicator */}
          {isRecording && (
            <div className="w-64 space-y-2">
              <div className="flex justify-between text-xs text-gray-600">
                <span>éŸ³é‡</span>
                <span>{(volumeLevel * 100).toFixed(0)}%</span>
              </div>
              <div className="w-full h-4 bg-gray-200 rounded-full overflow-hidden">
                <div
                  className={`h-full transition-all duration-100 ${
                    volumeLevel > 0.01 ? 'bg-green-500' : 'bg-yellow-500'
                  }`}
                  style={{ width: `${Math.min(volumeLevel * 100, 100)}%` }}
                />
              </div>
              <p className="text-xs text-center text-gray-500">
                {volumeLevel < 0.01 ? 'âš ï¸ éŸ³é‡éä½ï¼Œè«‹æé«˜éŸ³é‡' : 'âœ… éŸ³é‡æ­£å¸¸'}
              </p>
            </div>
          )}
        </div>

        {/* Status & Messages */}
        <div className="space-y-4">
          {error && (
            <div className="p-4 bg-red-50 border border-red-200 rounded-lg">
              <p className="text-red-600">âŒ {error}</p>
            </div>
          )}

          {transcript && (
            <div className="p-4 bg-blue-50 border border-blue-200 rounded-lg">
              <p className="text-sm text-gray-600 mb-1">ä½ èªªï¼š</p>
              <p className="text-gray-900">{transcript}</p>
            </div>
          )}

          {aiResponse && (
            <div className="p-4 bg-green-50 border border-green-200 rounded-lg">
              <p className="text-sm text-gray-600 mb-1">AI å›æ‡‰ï¼š</p>
              <p className="text-gray-900">{aiResponse}</p>
            </div>
          )}
        </div>

        {/* Instructions */}
        <div className="text-center text-sm text-gray-500 space-y-2">
          <p>ğŸ’¡ æŒ‰ä½éº¥å…‹é¢¨æŒ‰éˆ•èªªè©±ï¼Œæ”¾é–‹å¾Œç­‰å¾… AI å›æ‡‰</p>
          <p>âŒ¨ï¸ å¿«æ·éµï¼šæŒ‰ä½ <kbd className="px-2 py-1 bg-gray-200 rounded text-gray-700 font-mono">ç©ºç™½éµ</kbd> é–‹å§‹éŒ„éŸ³</p>
          <p>ğŸ¤ å³æ™‚éŸ³é‡æŒ‡ç¤ºå™¨æœƒé¡¯ç¤ºæ‚¨çš„èªªè©±éŸ³é‡</p>
          <p className="mt-2">ğŸ¯ ç›®æ¨™ï¼šé©—è­‰èªéŸ³å°è©±æ ¸å¿ƒæµç¨‹</p>
        </div>
      </div>
    </main>
  );
}
